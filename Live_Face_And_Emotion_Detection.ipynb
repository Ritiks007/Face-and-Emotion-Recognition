{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from statistics import mode\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = load_model('CustomResnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_input_size = (48,48)\n",
    "emotion_offsets = (20,40)\n",
    "frame_window = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_window = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "emotions = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_image(gray_img):\n",
    "    return gray_img/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(face_coordinates, image_array, color):\n",
    "    x, y, w, h = face_coordinates\n",
    "    cv2.rectangle(image_array, (x, y), (x + w, y + h),color ,thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,\n",
    "                                                font_scale=2, thickness=2):\n",
    "    x, y = coordinates[:2]\n",
    "    cv2.putText(image_array, text, (x + x_offset, y + y_offset),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    bgr_image = video_capture.read()[1]\n",
    "    gray_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2RGB)\n",
    "    faces = face_detection.detectMultiScale(gray_image,1.3,5)\n",
    "    \n",
    "    for face_coordinates in faces:\n",
    "        x1,x2,y1,y2 = apply_offsets(face_coordinates,emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face,(emotion_input_size))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        gray_face = normalise_image(gray_face)\n",
    "        gray_face = np.expand_dims(gray_face,0)\n",
    "        gray_face = np.expand_dims(gray_face,-1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_label = emotions[emotion_label_arg]\n",
    "        emotion_window.append(emotion_label)\n",
    "        \n",
    "        if(len(emotion_window))> frame_window:\n",
    "           emotion_window.pop(0)\n",
    "        try:\n",
    "           emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "           continue\n",
    "           \n",
    "        draw_bounding_box(face_coordinates,rgb_image,(0,255,0))\n",
    "        draw_text(face_coordinates,rgb_image,emotion_mode,(255,255,0),0,-45,1,1)\n",
    "    \n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "           \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "           \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv2.imread('test3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('window_frame')\n",
    "gray_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2GRAY)\n",
    "rgb_image = cv2.cvtColor(bgr_image,cv2.COLOR_BGR2RGB)\n",
    "faces = face_detection.detectMultiScale(gray_image,1.3,5)\n",
    "for face_coordinates in faces:\n",
    "    x1,x2,y1,y2 = apply_offsets(face_coordinates,emotion_offsets)\n",
    "    gray_face = gray_image[y1:y2, x1:x2]\n",
    "    try:\n",
    "        gray_face = cv2.resize(gray_face,(emotion_input_size))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    gray_face = normalise_image(gray_face)\n",
    "    gray_face = np.expand_dims(gray_face,0)\n",
    "    gray_face = np.expand_dims(gray_face,-1)\n",
    "    emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "    emotion_probability = np.max(emotion_prediction)\n",
    "    emotion_label_arg = np.argmax(emotion_prediction)\n",
    "    emotion_label = emotions[emotion_label_arg]\n",
    "#     emotion_window.append(emotion_label)\n",
    "\n",
    "#     if(len(emotion_window))> frame_window:\n",
    "#        emotion_window.pop(0)\n",
    "#     try:\n",
    "#        emotion_mode = mode(emotion_window)\n",
    "#     except:\n",
    "#        continue\n",
    "\n",
    "    draw_bounding_box(face_coordinates,rgb_image,(0,255,0))\n",
    "    draw_text(face_coordinates,rgb_image,emotion_label,(255,255,0),0,-45,1,1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "\n",
    "    if cv2.waitKey(10000) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnets_utils import *\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589/3589 [==============================] - 1s 415us/step\n",
      "Loss = 1.033392020414785\n",
      "Test Accuracy = 0.6288659572601318\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('28577_36420_bundle_archive/fer2013.csv')\n",
    "test_set = dataset[dataset['Usage']=='PrivateTest']\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "X_test,Y_test = test_set['pixels'],test_set['emotion']\n",
    "X_test2 = np.zeros((X_test.shape[0],48*48)) \n",
    "for i in range(X_test2.shape[0]):\n",
    "    p = X_test[i].split(' ')\n",
    "    for j in range(X_test2.shape[1]):\n",
    "        X_test2[i][j] = int(p[j])\n",
    "Y_test2 = np.array(Y_test)\n",
    "X_test3 = X_test2.reshape(X_test2.shape[0],48,48)\n",
    "Y_test = Y_test2\n",
    "X_test = X_test3\n",
    "\n",
    "X_test = X_test/255.0\n",
    "Y_test = convert_to_one_hot(Y_test, 7).T\n",
    "X_test = X_test.reshape(X_test.shape[0],48,48,1)\n",
    "emotion_classifier = load_model('CustomResnet.h5')\n",
    "preds = emotion_classifier.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchNormalization' object has no attribute 'uses_learning_phase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ffe0375f543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memotion_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bn_conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchNormalization' object has no attribute 'uses_learning_phase'"
     ]
    }
   ],
   "source": [
    "emotion_classifier.get_layer('bn_conv1').uses_learning_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
